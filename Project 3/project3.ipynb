{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"all_scripts_raw.json\", \"r\")\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['DS9', 'TOS', 'TAS', 'TNG', 'VOY', 'ENT'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DS9</th>\n",
       "      <th>TOS</th>\n",
       "      <th>TAS</th>\n",
       "      <th>TNG</th>\n",
       "      <th>VOY</th>\n",
       "      <th>ENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>episode 0</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - The Ca...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\nThe Voyager Transcripts - Caretaker\\...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Broke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - The Ma...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Voyager Transcripts - Parallax...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Fight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 2</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - Charli...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Voyager Transcripts - Time and...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Stran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 3</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - Where ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nThe Voyager Transcripts - Phage\\...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Unexp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode 4</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - The Na...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Voyager Transcripts - The Clou...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Terra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         DS9  \\\n",
       "episode 0  \\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...   \n",
       "episode 1  \\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...   \n",
       "episode 2  \\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...   \n",
       "episode 3  \\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...   \n",
       "episode 4  \\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts - ...   \n",
       "\n",
       "                                                         TOS  \\\n",
       "episode 0  \\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - The Ca...   \n",
       "episode 1  \\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - The Ma...   \n",
       "episode 2  \\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - Charli...   \n",
       "episode 3  \\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - Where ...   \n",
       "episode 4  \\n\\n\\n\\n\\n\\nThe Star Trek Transcripts - The Na...   \n",
       "\n",
       "                                                         TAS  \\\n",
       "episode 0  \\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...   \n",
       "episode 1  \\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...   \n",
       "episode 2  \\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...   \n",
       "episode 3  \\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...   \n",
       "episode 4  \\n\\n\\n\\n\\n\\nThe Animated Star Trek Transcripts...   \n",
       "\n",
       "                                                         TNG  \\\n",
       "episode 0  \\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...   \n",
       "episode 1  \\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...   \n",
       "episode 2  \\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...   \n",
       "episode 3  \\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...   \n",
       "episode 4  \\n\\n\\n\\n\\n\\nThe Next Generation Transcripts - ...   \n",
       "\n",
       "                                                         VOY  \\\n",
       "episode 0  \\n\\n\\n\\n\\nThe Voyager Transcripts - Caretaker\\...   \n",
       "episode 1  \\n\\n\\n\\n\\n\\nThe Voyager Transcripts - Parallax...   \n",
       "episode 2  \\n\\n\\n\\n\\n\\nThe Voyager Transcripts - Time and...   \n",
       "episode 3  \\n\\n\\n\\n\\n\\n\\nThe Voyager Transcripts - Phage\\...   \n",
       "episode 4  \\n\\n\\n\\n\\n\\nThe Voyager Transcripts - The Clou...   \n",
       "\n",
       "                                                         ENT  \n",
       "episode 0  \\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Broke...  \n",
       "episode 1  \\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Fight...  \n",
       "episode 2  \\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Stran...  \n",
       "episode 3  \\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Unexp...  \n",
       "episode 4  \\n\\n\\n\\n\\n\\nThe Enterprise Transcripts - Terra...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.keys())\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for series in df:\n",
    "    for ep in df[series].index:\n",
    "        if type(df[series][ep]) == str:\n",
    "            df[series][ep] = re.sub(r'\\s+', ' ', df[series][ep].strip())\n",
    "\n",
    "# df['DS9'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scenes = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a regex test ',\n",
       " '[bracket] other text ',\n",
       " '[Ops] is here ',\n",
       " '[test] more text [OC] this is comms [on screen] yo']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"This is a regex test [bracket] other text [Ops] is here [test] more text [OC] this is comms [on screen] yo\"\n",
    "scene_regex = re.compile(r'(?=[\\[]+(?!OC)+(?!on))')\n",
    "scene_regex.split(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for series in df_scenes:\n",
    "    for ep in df_scenes[series].index:\n",
    "        if type(df[series][ep]) == str:\n",
    "            df_scenes[series][ep] = scene_regex.split(df_scenes[series][ep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Deep Space Nine Transcripts - Past Prologue Past Prologue Stardate: Unknown Original Airdate: 11 Jan, 1993 ',\n",
       " \"[Replimat] GARAK: It's Doctor Bashir, isn't it? Of course it is. May I introduce myself? BASHIR: Er, yes, yes, of course. GARAK: My name is Garak. Cardassian by birth, obviously. The only one of us left on this station, as a matter of fact, so I do appreciate making new friends whenever I can. You are new to this station, I believe. BASHIR: I am, yes. Though, though I understand you've been here quite a while. GARAK: Ah, you know of me then. BASHIR: Would you care for some of this Tarkalean tea? It's very good. GARAK: What a thoughtful young man. How nice that we've met. BASHIR: You know, some people say that you remained on DS Nine as the eyes and ears of your fellow Cardassians. GARAK: You don't say? Doctor, you're not intimating that I'm considered some sort of spy, are you? BASHIR: I wouldn't know, sir. GARAK: Ah. An open mind. The essence of intellect. As you may also know, I have a clothing shop nearby, so if you should require any apparel, or merely wish, as I do, for a bit of enjoyable company now and then, I'm at your disposal, Doctor. BASHIR: You're very kind, Mister Garak. GARAK: Oh, it's just Garak. Plain, simple Garak. Now, good day to you, Doctor. I'm so glad to have made such an interesting new friend today. \",\n",
       " \"[Ops] BASHIR: You won't never believe who just sat down next to me at the Replimat O'BRIEN: Major, upper pylon three'll be shut down for maintenance for forty-eight hours. BASHIR: The spy! Garak, the Cardassian. SISKO: We don't know for a fact Garak's a spy, Doctor. BASHIR: He is. You should have heard him. He introduced himself and he struck up conversation just like that. he was making contact with me, with me of all people. DAX: What do you think he might want from you, Julian? BASHIR: I don't know. Federation medical secrets? Rest assured they're safe with me, Commander. SISKO: I'm sure they are, Doctor Bashir. BASHIR: In fact, Chief O'Brien, I think you should place a monitoring device on me. Well, just in case he's up to something? SISKO: I don't think that'll be necessary, Doctor. Just be very cautious when you're around him. KIRA: (who's had a hair cut) Commander? We've got a small craft taking evasive action. Cardassian war vessel in pursuit. SISKO: On screen. KIRA: That's Bajoran. That damned Cardassian's firing at a Bajoran scout ship in Bajoran space! SISKO: Mister O'Brien? O'BRIEN: Confirmed, sir. They've crossed into Bajoran space. SISKO: Open a hailing frequency to the Cardassians. O'BRIEN: Channel open. SISKO: Cardassian vessel, you are violating Bajoran space. Break off your pursuit. Repeat, break off now. O'BRIEN: No reply from the Cardassians. The Bajoran vessel is hailing us. SISKO: Open the channel. TAHNA [OC]: Space station, do you read? Space station O'BRIEN: We can only get audio, Commander. SISKO: This is Benjamin Sisko, Starfleet Commander of the station. Who are you? Why are they pursuing? TAHNA [OC]: Please! Repeating request for emergency docking! Please! DAX: The Bajoran scout ship is badly damaged. Structural integrity is failing. (The little ship gets hit again) DAX: He's breaking up. SISKO: Get him out, Mister O'Brien. O'BRIEN: Aye, sir. (KaBOOM, and a crouching figure is beamed in) BASHIR: Medical assistance to Ops. TAHNA: My name is Tahna Los. Request political asylum. Kira? (after the opening credits, medics are wheeling Tahna away) KIRA: His name is Tahna Los. We fought together in the underground. O'BRIEN: Commander, the Cardassians are hailing us. KIRA: Now they want to talk. O'BRIEN: They're hopping mad. SISKO: Open the channel, Mister O'Brien. GUL DANAR [on viewscreen]: Federation Commander, you've taken aboard a known criminal. You will turn him over to us. SISKO: He has requested asylum. GUL DANAR [on viewscreen]: You have not granted it. SISKO: To be honest, I haven't decided yet. GUL DANAR [on viewscreen]: He is Kohn-Ma! Even the Bajorans would not grant his kind asylum. He has committed heinous crimes against the Cardassian people and I demand you release him to our custody. SISKO: I'll investigate the matter immediately. In the interim, if you'd care to dock your vessel, I'll be glad to hear an explanation for having violated Bajoran space and threatened a Federation facility. GUL DANAR [on viewscreen]: We have made no threat to your facility. SISKO: I stand corrected. Sisko out. (transmission ends) SISKO: The Major and I will be at the Infirmary. I'd like some time to talk with this fellow Tahna. When Gul Danar comes in, it'd be nice if we had a few docking regulations to keep him outside a while. O'BRIEN: Understood. (Sisko and Kira get onto the lift) KIRA: You're not seriously considering handing Tahna over to the Cardassians? \"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scenes['DS9'][1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.words = []\n",
    "        self.word_count = []\n",
    "        self.df = pd.DataFrame(columns=['word', 'count'])\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word in self.words:\n",
    "            self.word_count[self.words.index(word)] += 1\n",
    "        else:\n",
    "            self.words.append(word)\n",
    "            self.word_count.append(1)\n",
    "\n",
    "    def get_word_count(self, word):\n",
    "        if word in self.words:\n",
    "            return self.word_count[self.words.index(word)]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def rebuild_dataframe(self):\n",
    "        self.df = pd.DataFrame({\"word\": vocab.words, \"count\": vocab.word_count})\n",
    "        self.df.sort_values('count', ascending=False, inplace=True)\n",
    "\n",
    "    def get_as_dataframe(self):\n",
    "        return self.df\n",
    "\n",
    "    def get_total_size(self):\n",
    "        return sum(self.word_count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def get_words(self, sorted=True):\n",
    "        # if sorted:\n",
    "        #     return self.words.sort_values('count', ascending=False)\n",
    "        # else:\n",
    "        return self.words\n",
    "\n",
    "    def tokenize(s):\n",
    "        if type(s) == str:\n",
    "            s = find_tokens.findall(s.lower().strip())\n",
    "\n",
    "        tok = []\n",
    "        for t in s:\n",
    "            if t.isdigit():\n",
    "                for d in re.findall(r'(\\d)', t):\n",
    "                    tok.append(tokens[d])\n",
    "            elif t in tokens:\n",
    "                tok.append(tokens[t])\n",
    "            else:\n",
    "                tok.append(tokens[\"<unk>\"])\n",
    "\n",
    "        return tok\n",
    "\n",
    "\n",
    "    def inv_tokenize(s):\n",
    "        return \" \".join([words_from_token[t] for t in s])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words = re.compile(r'[\\s\\[\\]\\(\\),.?!:]')\n",
    "split_sentence = re.compile(r'(?<=[?!.])(?![\\d]+)')\n",
    "find_tokens = re.compile(r\"[\\w']+|[.,!?:;\\[\\]\\(\\)]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words...\n",
      "Finished loading DS9\n",
      "Finished loading TOS\n",
      "Finished loading TAS\n",
      "Finished loading TNG\n",
      "Finished loading VOY\n",
      "Finished loading ENT\n",
      "Unique words: 43493\n",
      "Generating Dataframe...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# vocab = Vocab()\n",
    "\n",
    "# print(\"Loading words...\")\n",
    "\n",
    "# for series in df:\n",
    "#     for ep in df[series]:\n",
    "#         if type(ep) != str:\n",
    "#             continue\n",
    "#         word_split = find_tokens.findall(ep.lower().strip()) #re.split(r'[\\s\\[\\]\\(\\),.?!:]', ep.lower().strip())\n",
    "        \n",
    "#         words = [w.strip() for w in word_split if w != '' and not w.isdigit()]\n",
    "#         nums = [w for w in word_split if w.isdigit()]\n",
    "#         digits = []\n",
    "#         for n in nums:\n",
    "#             l = re.split(r'(\\d)', n)\n",
    "#             for d in l:\n",
    "#                 if d.isdigit():\n",
    "#                     digits.append(d)\n",
    "\n",
    "#         for w in words:\n",
    "#             vocab.add_word(w)\n",
    "\n",
    "#         for w in digits:\n",
    "#             vocab.add_word(w)\n",
    "#     print(f\"Finished loading {series}\")\n",
    "\n",
    "# print(\"Unique words:\", len(vocab))\n",
    "# print(\"Generating Dataframe...\")\n",
    "# vocab.rebuild_dataframe()\n",
    "# vocab.get_as_dataframe().to_csv('all_vocab.csv')\n",
    "# print(\"Done!\")\n",
    "\n",
    "# vocab.get_as_dataframe().head()\n",
    "# re.split(r'[\\s\\[\\]\\(\\),.?!:]+', df_scenes['DS9'][1][1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_df = pd.read_csv(\"all_vocab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4817376 4905882 0.9819592073351948\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>.</td>\n",
       "      <td>409502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>:</td>\n",
       "      <td>270065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>,</td>\n",
       "      <td>169143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>146906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>to</td>\n",
       "      <td>110449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token word   count\n",
       "0      8    .  409502\n",
       "1      7    :  270065\n",
       "2     12    ,  169143\n",
       "3      0  the  146906\n",
       "4     38   to  110449"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_df.columns = ['token', 'word', 'count']\n",
    "voc_max_size = voc_df['count'].sum()\n",
    "voc_df = voc_df[:12000]\n",
    "voc_size = voc_df['count'].sum()\n",
    "\n",
    "print(voc_size, voc_max_size, voc_size/voc_max_size)\n",
    "voc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12005\n"
     ]
    }
   ],
   "source": [
    "extra_tokens = [\"<pad>\", \"<unk>\", \"<mask>\", \"<start>\", \"<end>\"]\n",
    "\n",
    "tokens = {t: i for i, t in enumerate(extra_tokens + voc_df['word'].tolist())}\n",
    "words_from_token = {i: t for i, t in enumerate(extra_tokens + voc_df['word'].tolist())}\n",
    "voc_size = len(tokens)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;pad&gt;</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;mask&gt;</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;start&gt;</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;end&gt;</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "<pad>    0\n",
       "<unk>    1\n",
       "<mask>   2\n",
       "<start>  3\n",
       "<end>    4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = pd.DataFrame(columns=['token', 'word']).from_dict(tokens, orient='index')\n",
    "tok.to_csv('tokens.csv')\n",
    "tok.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15408/3012541662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords_from_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<start> this is a test <end>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15408/3012541662.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(\\d)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    if type(s) == str:\n",
    "        s = find_tokens.findall(s.lower().strip())    \n",
    "    \n",
    "    tok = []\n",
    "    for t in s:\n",
    "        if t.isdigit():\n",
    "            for d in re.findall(r'(\\d)', t):\n",
    "                tok.append(tokens[d])\n",
    "        elif t in tokens:\n",
    "            tok.append(tokens[t])\n",
    "        else:\n",
    "            tok.append(tokens[\"<unk>\"])\n",
    "\n",
    "    return tok\n",
    "\n",
    "def inv_tokenize(s):\n",
    "    return \" \".join([words_from_token[t] for t in s])\n",
    "\n",
    "test = tokenize(\"<start> this is a test <end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_df.to_csv(\"tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12005"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sentence = re.compile(r'(?<=[?!.])(?![\\d]+)')\n",
    "find_tokens = re.compile(r\"[\\w']+|[.,!?:;\\[\\]\\(\\)]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the deep space nine transcripts - emissary emissary stardate: 46379.1 original airdate: 3 jan, 1993 on stardate 43997, captain jean-luc picard of the federation starship enterprise was kidnapped for six days by an invading force known as the borg. surgically altered, he was forced to lead an assault on starfleet at wolf 359.\n",
      "['the deep space nine transcripts - emissary emissary stardate: 46379.1 original airdate: 3 jan, 1993 on stardate 43997, captain jean-luc picard of the federation starship enterprise was kidnapped for six days by an invading force known as the borg.', ' surgically altered, he was forced to lead an assault on starfleet at wolf 359.', '']\n"
     ]
    }
   ],
   "source": [
    "s = df_scenes['DS9'][0][0].strip().lower()\n",
    "print(s)\n",
    "print(split_sentence.split(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'deep', 'space', 'nine', 'transcripts', 'emissary', 'emissary', 'stardate', ':', '46379', '.', '1', 'original', 'airdate', ':', '3', 'jan', ',', '1993', 'on', 'stardate', '43997', ',', 'captain', 'jean', 'luc', 'picard', 'of', 'the', 'federation', 'starship', 'enterprise', 'was', 'kidnapped', 'for', 'six', 'days', 'by', 'an', 'invading', 'force', 'known', 'as', 'the', 'borg', '.', 'surgically', 'altered', ',', 'he', 'was', 'forced', 'to', 'lead', 'an', 'assault', 'on', 'starfleet', 'at', 'wolf', '359', '.']\n",
      "[3, 8, 713, 291, 293, 664, 1746, 1746, 357, 6, 268, 247, 345, 400, 222, 5, 196, 523, 674, 6, 345, 4068, 7, 196, 222, 222, 345, 25, 357, 268, 345, 222, 222, 400, 7, 46, 1008, 1024, 48, 14, 8, 284, 594, 188, 39, 5033, 30, 325, 349, 99, 65, 9060, 676, 688, 53, 8, 351, 5, 4]\n",
      "<start> the deep space nine transcripts emissary emissary stardate : 4 6 3 7 9 . 1 original airdate : 3 jan , 1 9 9 3 on stardate 4 3 9 9 7 , captain jean luc picard of the federation starship enterprise was kidnapped for six days by an invading force known as the borg . <end>\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "print(find_tokens.findall(s))\n",
    "for sentence in split_sentence.split(s):\n",
    "    sent = tokenize(sentence)\n",
    "    words.extend(sent)\n",
    "    sent.insert(0, tokens['<start>'])\n",
    "    sent.append(tokens[\"<end>\"])\n",
    "    print(sent)\n",
    "    print(inv_tokenize(sent))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building sentence corpus...\n",
      "Finished loading DS9\n",
      "Finished loading TOS\n",
      "Finished loading TAS\n",
      "Finished loading TNG\n",
      "Finished loading VOY\n",
      "Finished loading ENT\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sentences = {}\n",
    "raw_sentences = []\n",
    "words = []\n",
    "print(\"building sentence corpus...\")\n",
    "for series in df_scenes:\n",
    "    sentences[series] = []\n",
    "    for ep in df_scenes[series]:\n",
    "        if type(ep) != list:\n",
    "            continue\n",
    "\n",
    "        for scene in ep:\n",
    "            if type(scene) != str:\n",
    "                continue\n",
    "            # words.extend(split_words.split(scene.strip().lower()))\n",
    "            for s in split_sentence.split(scene.strip().lower()):\n",
    "                raw_sentences.append(s)     \n",
    "                # sent = tokenize(s)\n",
    "                # sent.insert(0, tokens['<start>'])\n",
    "                # sent.append(tokens[\"<end>\"])\n",
    "                # if len(sent) > 2:\n",
    "                    # sentences[series].append(sent)\n",
    "\n",
    "                    # words.extend(sent)\n",
    "                # print(find_tokens.findall(s.strip().lower()))\n",
    "                \n",
    "\n",
    "         \n",
    "    print(f\"Finished loading {series}\")\n",
    "\n",
    "print(\"Done!\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5891248\n",
      "[3, 8, 713, 291, 293, 664, 1746, 1746, 357, 6, 268, 247, 345, 400, 222, 5, 196, 523, 674, 6, 345, 4068, 7, 196, 222, 222, 345, 25, 357, 268, 345, 222, 222, 400, 7, 46, 1008, 1024, 48, 14, 8, 284, 594, 188, 39, 5033, 30, 325, 349, 99, 65, 9060, 676, 688, 53, 8, 351, 5, 4, 3, 7761, 2011, 7, 43, 39, 1338, 9, 1033, 65, 2682, 25, 226, 56, 6668, 345, 355, 222, 5, 4, 3, 18, 11651, 92, 19, 9583, 18, 25, 229, 19, 6, 1632, 17, 4102, 5, 4, 3, 10, 57, 4914, 31]\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if w != '']\n",
    "print(len(words))\n",
    "print(words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> the deep space nine transcripts emissary emissary stardate : 4 6 3 7 9 . 1 original airdate : 3 jan , 1 9 9 3 on stardate 4 3 9 9 7 , captain jean luc picard of the federation starship enterprise was kidnapped for six days by an invading force known as the borg . <end>\n",
      "max length: 78\n"
     ]
    }
   ],
   "source": [
    "print(inv_tokenize(sentences['DS9'][0]))\n",
    "\n",
    "max_length = 0\n",
    "for series in sentences:\n",
    "    for sent in sentences[series]:\n",
    "        if len(sent) > max_length:\n",
    "            max_length = len(sent)\n",
    "print(\"max length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122505/122505 [08:39<00:00, 235.72it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_skip_grams(sentences, window_size, num_ns, vocab_size, sampling_factor=1e-3, SEED=0):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    \n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(\n",
    "        vocab_size, sampling_factor=sampling_factor)\n",
    "\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(sentence, vocabulary_size=vocab_size, \n",
    "            window_size=window_size, sampling_table=sampling_table, negative_samples=num_ns)\n",
    "\n",
    "        for target_word, context_words in skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_words], dtype=tf.int64), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=SEED,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "                negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat(\n",
    "                      [context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    \n",
    "    return targets, contexts, labels\n",
    "\n",
    "\n",
    "\n",
    "sg = generate_skip_grams(sentences['DS9'], window_size=1, num_ns=3, vocab_size=len(tokens), sampling_factor=1e-5 , SEED=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1138984,) (1138984, 4) (1138984, 4)\n"
     ]
    }
   ],
   "source": [
    "targets = np.array(sg[0])\n",
    "contexts = np.array(sg[1])[:,:,0]\n",
    "labels = np.array(sg[2])\n",
    "print(targets.shape, contexts.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (((1024,), (1024, 4)), (1024, 4)), types: ((tf.int32, tf.int64), tf.int64)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "skipgram_dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "skipgram_dataset = skipgram_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "skipgram_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 4)), (1024, 4)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "skipgram_dataset = skipgram_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(skipgram_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00996828 0.00996828 0.01731655 ... 1.         1.         1.        ]\n",
      "[[869, 472], [420, 12146], [869, 2855], [869, 9349], [869, 11], [869, 9933], [869, 26], [26, 8479], [26, 16], [420, 11066], [420, 2269], [26, 9020], [26, 11], [420, 26], [869, 2042], [420, 16], [869, 16], [869, 8225], [420, 11], [26, 4442], [869, 9598], [26, 1618], [26, 420], [869, 10861], [869, 5309], [26, 196], [26, 9912], [26, 869], [26, 8820], [26, 7178], [420, 1229], [420, 2221], [420, 859]]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "\n",
    "# Build the sampling table for vocab_size tokens.\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(tokens), sampling_factor=1e-4)\n",
    "print(sampling_table)\n",
    "\n",
    "pos_skip_grams, is_neg = tf.keras.preprocessing.sequence.skipgrams(test, vocabulary_size=len(tokens),\n",
    "    window_size=window_size, negative_samples=2, sampling_table=sampling_table)\n",
    "print(pos_skip_grams)\n",
    "print(is_neg)\n",
    "print(len(pos_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, samples):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                             embedding_dim,\n",
    "                                             input_length=1,\n",
    "                                             name=\"w2v_embedding\")\n",
    "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                              embedding_dim,\n",
    "                                              input_length=samples)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots\n",
    "\n",
    "  def negative_sampling_loss(x_logit, y_true):\n",
    "    \"\"\"\n",
    "    Compute the loss for negative sampling.\n",
    "    \"\"\"\n",
    "    # x_logit: (batch, context)\n",
    "    # y_true: (batch, context)\n",
    "    x_logit = tf.cast(x_logit, tf.float32)\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=x_logit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(len(tokens), embedding_dim, 4)\n",
    "word2vec.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True) , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1112/1112 [==============================] - 19s 17ms/step - loss: 1.0524 - accuracy: 0.5637\n",
      "Epoch 2/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.8475 - accuracy: 0.6463\n",
      "Epoch 3/20\n",
      "1112/1112 [==============================] - 18s 17ms/step - loss: 0.7854 - accuracy: 0.6735\n",
      "Epoch 4/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.7288 - accuracy: 0.7024\n",
      "Epoch 5/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.6701 - accuracy: 0.7343\n",
      "Epoch 6/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.6087 - accuracy: 0.7684\n",
      "Epoch 7/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.5458 - accuracy: 0.8039\n",
      "Epoch 8/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.4839 - accuracy: 0.8378\n",
      "Epoch 9/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.4251 - accuracy: 0.8680\n",
      "Epoch 10/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.3710 - accuracy: 0.8935\n",
      "Epoch 11/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.3226 - accuracy: 0.9142\n",
      "Epoch 12/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.2799 - accuracy: 0.9307\n",
      "Epoch 13/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.2428 - accuracy: 0.9439\n",
      "Epoch 14/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.2108 - accuracy: 0.9541\n",
      "Epoch 15/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.1833 - accuracy: 0.96200s - loss: 0.1833 - accuracy: \n",
      "Epoch 16/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.1599 - accuracy: 0.9680\n",
      "Epoch 17/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.1400 - accuracy: 0.9725\n",
      "Epoch 18/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.1232 - accuracy: 0.9760\n",
      "Epoch 19/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.1090 - accuracy: 0.9786\n",
      "Epoch 20/20\n",
      "1112/1112 [==============================] - 18s 16ms/step - loss: 0.0970 - accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs/word2vec', histogram_freq=1)\n",
    "H = word2vec.fit(skipgram_dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12178, 128)\n",
      "12178\n"
     ]
    }
   ],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "print(weights.shape)\n",
    "print(len(tokens))\n",
    "\n",
    "emb = pd.DataFrame([weights[i] for i in range(len(tokens))], index=tokens)\n",
    "emb.to_csv('word2vec_embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;pad&gt;</th>\n",
       "      <td>0.026874</td>\n",
       "      <td>-0.049804</td>\n",
       "      <td>-0.040913</td>\n",
       "      <td>-0.042245</td>\n",
       "      <td>0.026275</td>\n",
       "      <td>-0.015231</td>\n",
       "      <td>-0.045740</td>\n",
       "      <td>-0.001371</td>\n",
       "      <td>-0.027052</td>\n",
       "      <td>-0.028193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023904</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.020214</td>\n",
       "      <td>0.024352</td>\n",
       "      <td>0.041932</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.038524</td>\n",
       "      <td>0.035024</td>\n",
       "      <td>0.036776</td>\n",
       "      <td>0.036592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;sep&gt;</th>\n",
       "      <td>-0.808696</td>\n",
       "      <td>1.793078</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>1.399279</td>\n",
       "      <td>1.022748</td>\n",
       "      <td>-0.577521</td>\n",
       "      <td>-1.264128</td>\n",
       "      <td>0.236296</td>\n",
       "      <td>0.445305</td>\n",
       "      <td>-0.354724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.791864</td>\n",
       "      <td>-0.377173</td>\n",
       "      <td>0.017149</td>\n",
       "      <td>1.202706</td>\n",
       "      <td>0.216141</td>\n",
       "      <td>-0.858716</td>\n",
       "      <td>-0.348016</td>\n",
       "      <td>-0.611267</td>\n",
       "      <td>-0.478441</td>\n",
       "      <td>-0.487538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "      <td>0.573022</td>\n",
       "      <td>-0.099768</td>\n",
       "      <td>-0.759066</td>\n",
       "      <td>0.123787</td>\n",
       "      <td>-0.352095</td>\n",
       "      <td>-0.504453</td>\n",
       "      <td>0.241041</td>\n",
       "      <td>0.639629</td>\n",
       "      <td>0.160775</td>\n",
       "      <td>0.275783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362448</td>\n",
       "      <td>-0.212791</td>\n",
       "      <td>0.098707</td>\n",
       "      <td>0.039922</td>\n",
       "      <td>-0.369519</td>\n",
       "      <td>-0.508871</td>\n",
       "      <td>-0.292618</td>\n",
       "      <td>0.908240</td>\n",
       "      <td>0.949985</td>\n",
       "      <td>-0.134691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;mask&gt;</th>\n",
       "      <td>-0.048459</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>-0.002477</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>-0.002760</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>-0.028590</td>\n",
       "      <td>0.049159</td>\n",
       "      <td>-0.003574</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018194</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>-0.019113</td>\n",
       "      <td>-0.034744</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.043678</td>\n",
       "      <td>-0.033830</td>\n",
       "      <td>-0.007656</td>\n",
       "      <td>0.003803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-1.094318</td>\n",
       "      <td>0.385190</td>\n",
       "      <td>0.228293</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>-0.431485</td>\n",
       "      <td>-0.047274</td>\n",
       "      <td>-0.268959</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>-0.159678</td>\n",
       "      <td>0.691581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350256</td>\n",
       "      <td>1.107469</td>\n",
       "      <td>-0.719812</td>\n",
       "      <td>0.680427</td>\n",
       "      <td>1.287752</td>\n",
       "      <td>1.353465</td>\n",
       "      <td>0.164322</td>\n",
       "      <td>-0.064036</td>\n",
       "      <td>-0.717928</td>\n",
       "      <td>-0.396783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "<pad>   0.026874 -0.049804 -0.040913 -0.042245  0.026275 -0.015231 -0.045740   \n",
       "<sep>  -0.808696  1.793078  0.018985  1.399279  1.022748 -0.577521 -1.264128   \n",
       "<unk>   0.573022 -0.099768 -0.759066  0.123787 -0.352095 -0.504453  0.241041   \n",
       "<mask> -0.048459  0.021785 -0.002477  0.009309 -0.002760  0.001185 -0.028590   \n",
       ".      -1.094318  0.385190  0.228293  0.002123 -0.431485 -0.047274 -0.268959   \n",
       "\n",
       "             7         8         9    ...       118       119       120  \\\n",
       "<pad>  -0.001371 -0.027052 -0.028193  ...  0.023904 -0.000802 -0.020214   \n",
       "<sep>   0.236296  0.445305 -0.354724  ... -0.791864 -0.377173  0.017149   \n",
       "<unk>   0.639629  0.160775  0.275783  ...  0.362448 -0.212791  0.098707   \n",
       "<mask>  0.049159 -0.003574 -0.015082  ...  0.018194 -0.000781 -0.019113   \n",
       ".       0.009365 -0.159678  0.691581  ...  0.350256  1.107469 -0.719812   \n",
       "\n",
       "             121       122       123       124       125       126       127  \n",
       "<pad>   0.024352  0.041932  0.001481  0.038524  0.035024  0.036776  0.036592  \n",
       "<sep>   1.202706  0.216141 -0.858716 -0.348016 -0.611267 -0.478441 -0.487538  \n",
       "<unk>   0.039922 -0.369519 -0.508871 -0.292618  0.908240  0.949985 -0.134691  \n",
       "<mask> -0.034744  0.010155  0.020153  0.043678 -0.033830 -0.007656  0.003803  \n",
       ".       0.680427  1.287752  1.353465  0.164322 -0.064036 -0.717928 -0.396783  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(H.history['accuracy'])\n",
    "plt.plot(H.history['loss'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(sentences, open('tok_sentences.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices([0,1,2,3,4])\n",
    "for i in ds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122505/122505 [00:01<00:00, 102870.17it/s]\n",
      "100%|██████████| 58922/58922 [00:00<00:00, 102117.78it/s]\n",
      "100%|██████████| 8064/8064 [00:00<00:00, 100801.30it/s]\n",
      "100%|██████████| 112394/112394 [00:01<00:00, 99672.60it/s]\n",
      "100%|██████████| 128514/128514 [00:01<00:00, 102501.34it/s]\n",
      "100%|██████████| 62284/62284 [00:00<00:00, 103247.32it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 96\n",
    "\n",
    "padded_sentences = []\n",
    "following_sentences = []\n",
    "for series in sentences.keys():\n",
    "    for sentence in tqdm.tqdm(sentences[series]):\n",
    "        padded_sentence = [0] * seq_length\n",
    "        for i, word in enumerate(sentence):\n",
    "            if i >= seq_length:\n",
    "                break\n",
    "            padded_sentence[i] = word\n",
    "        padded_sentence = np.array(padded_sentence)\n",
    "        padded_sentences.append(padded_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492682"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sentences[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sent_ds = tf.data.Dataset.from_tensor_slices((padded_sentences[:-1], padded_sentences[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> the deep space nine transcripts emissary emissary stardate : 4 6 3 7 9 . 1 original airdate : 3 jan , 1 9 9 3 on stardate 4 3 9 9 7 , captain jean luc picard of the federation starship enterprise was kidnapped for six days by an invading force known as the borg . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<start> surgically altered , he was forced to lead an assault on starfleet at wolf 3 5 9 . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for s in padded_sent_ds.take(1):\n",
    "    print(inv_tokenize(s[0].numpy()))\n",
    "    print(inv_tokenize(s[1].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(padded_sent_ds, 'padded_sentences.tfrecord', compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   3    8  713  291  293  664 1746 1746  357    6  268  247  345  400\n",
      "  222    5  196  523  674    6  345 4068    7  196  222  222  345   25\n",
      "  357  268  345  222  222  400    7   46 1008 1024   48   14    8  284\n",
      "  594  188   39 5033   30  325  349   99   65 9060  676  688   53    8\n",
      "  351    5    4    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(96,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "record_sentences = tf.data.experimental.load('padded_sentences.tfrecord', compression='GZIP')\n",
    "\n",
    "for s in record_sentences.take(1):\n",
    "    print(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492683"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ds = tf.data.Dataset.from_tensor_slices(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "se = []\n",
    "for s in words:\n",
    "    # if s in tokens.keys():\n",
    "    #     se.append(tokens[s])\n",
    "    # else:\n",
    "    #     se.append(tokens['<unk>'])\n",
    "    se.append(s)\n",
    "    if(len(se) == 96):\n",
    "        sent.append(se)\n",
    "        se = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61367"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sent_ds = tf.data.Dataset.from_tensor_slices(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[    3     8   713   291   293   664  1746  1746   357     6   268   247\n",
      "   345   400   222     5   196   523   674     6   345  4068     7   196\n",
      "   222   222   345    25   357   268   345   222   222   400     7    46\n",
      "  1008  1024    48    14     8   284   594   188    39  5033    30   325\n",
      "   349    99    65  9060   676   688    53     8   351     5     4     3\n",
      "  7761  2011     7    43    39  1338     9  1033    65  2682    25   226\n",
      "    56  6668   345   355   222     5     4     3    18 11651    92    19\n",
      "  9583    18    25   229    19     6  1632    17  4102     5     4     3], shape=(96,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for s in comp_sent_ds.take(1):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(\n",
    "    comp_sent_ds, 'comp_sent.tfrecord', compression=\"GZIP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = [s for s in raw_sentences if len(s) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the deep space nine transcripts - emissary emissary stardate: 46379.1 original airdate: 3 jan, 1993 on stardate 43997, captain jean-luc picard of the federation starship enterprise was kidnapped for six days by an invading force known as the borg.',\n",
       " ' surgically altered, he was forced to lead an assault on starfleet at wolf 359.',\n",
       " '[saratoga - bridge] locutus [on viewscreen]: resistance is futile.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "vocab_length = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "def tokenize(s, max_len=128):\n",
    "    tok = tokenizer.encode(s)\n",
    "    return tf.constant(tok, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492686/492686 [00:57<00:00, 8635.14it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_gpt_tokens = []\n",
    "for s in tqdm.tqdm(raw_sentences):\n",
    "    raw_gpt_tokens.append(tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492686/492686 [00:17<00:00, 27867.82it/s]\n"
     ]
    }
   ],
   "source": [
    "combined_sentences = []\n",
    "scratch = []\n",
    "for s in tqdm.tqdm(raw_gpt_tokens):\n",
    "    if (len(scratch) + len(s)) < 128:\n",
    "        scratch = tf.concat([scratch, s], axis=0)\n",
    "        # print(scratch)\n",
    "    else:\n",
    "        scratch = tf.concat(\n",
    "                [scratch, [tokenizer.pad_token_id]*(128 - len(scratch))], axis=0)\n",
    "        s = tf.concat([s, [tokenizer.pad_token_id]*((128 - len(s)))], axis=0)\n",
    "        combined_sentences.append((scratch, s))\n",
    "        scratch = []\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
       " array([ 1169,  2769,  2272,  5193, 29351,   532,   795,   747,   560,\n",
       "          795,   747,   560,   336,   446,   378,    25,  6337, 29088,\n",
       "           13,    16,  2656,   257,  1447,   378,    25,   513, 42897,\n",
       "           11,  9656,   319,   336,   446,   378,  5946, 39647,    11,\n",
       "        10654,   474, 11025,    12,    75,  1229,  8301,   446,   286,\n",
       "          262, 36986, 48744, 13953,   373, 23384,   329,  2237,  1528,\n",
       "          416,   281, 33827,  2700,  1900,   355,   262,   275,  2398,\n",
       "           13, 19797,  1146, 14294,    11,   339,   373,  4137,   284,\n",
       "         1085,   281,  4641,   319,  3491, 33559,   379, 17481, 41934,\n",
       "           13,    58,    82, 34174, 10949,   532,  7696,    60,  1179,\n",
       "          315,   385,   685,   261,  1570,  9612,  5974,  6625,   318,\n",
       "        35322,    13,   345,   481, 35526,   534,  3777,   290, 27675,\n",
       "          514,   284,  6567,  6632,  6632,   530,    13,   611,   345,\n",
       "         2230,   284, 22432,    11,   356,   481,  4117,   345,    13,\n",
       "        50257, 50257])>,\n",
       " <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
       " array([10654,    25,   357,    64, 24477,  5171,     8,  2266,  7995,\n",
       "           13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257])>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences_ds = tf.data.Dataset.from_tensor_slices(combined_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(\n",
    "    raw_sentences_ds, 'sentences_combined_gpttokens.tfrecord', compression=\"GZIP\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdcdff60b67b05b67ad0ae04d8e5c3b481a43b83a699826d48039a1889185218"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
